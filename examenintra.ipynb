{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# leba3207\n",
        "from unicodedata import category\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import difflib\n",
        "from functools import partial"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chargement des donn\u00e9es des diff\u00e9rents fichiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "data_folder = 'data/'\n",
        "\n",
        "journal_file = 'api_journal11-13-17.csv'\n",
        "price_file = 'api_price11-13-17.csv'\n",
        "influence_file = 'estimated-article-influence-scores-2015.csv'\n",
        "\n",
        "journal = pd.read_csv(data_folder + journal_file, sep=',', encoding='latin1')\n",
        "price = pd.read_csv(data_folder + price_file, sep=',', index_col=0)\n",
        "influence = pd.read_csv(data_folder + influence_file, sep=',', index_col=0)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def get_uniqueness_attributes(table):\n",
        "    return table.nunique(axis=0)\n",
        "\n",
        "\n",
        "def get_ratio_missing_values(table):\n",
        "    return table.isnull().sum() * 100 / len(table)\n",
        "\n",
        "\n",
        "def get_unique_values_of_attribute(table, header):\n",
        "    return table[header].unique()\n",
        "\n",
        "\n",
        "def lowercase_columns(table, headers):\n",
        "    for header in headers:\n",
        "        table[header] = table[header].str.lower()\n",
        "\n",
        "\n",
        "# TODO: include headers specification\n",
        "def get_df_duplicated_rows_dropped(table):\n",
        "    return table.drop_duplicates()\n",
        "\n",
        "\n",
        "def plot_categories_frequency(table, header):\n",
        "    fig, ax = plt.subplots()\n",
        "    table[header].value_counts()[0:5].plot(ax=ax, kind='bar')\n",
        "    plt.title(f'Fr\u00e9quence d\\'apparition des cat\u00e9gories de l\\'attribut {header}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_mean_price_per_year():\n",
        "    mean_price_per_year = {}\n",
        "    for index, p in price.iterrows():\n",
        "        year = p['date_stamp'].year\n",
        "        if year in mean_price_per_year:\n",
        "            mean_price_per_year[year] += p['price']\n",
        "        else:\n",
        "            mean_price_per_year[year] = p['price']\n",
        "\n",
        "    for year, value in mean_price_per_year.items():\n",
        "        mean_price_per_year[year] /= len(price[price['price'] != 0])\n",
        "    return {k: v for k, v in sorted(mean_price_per_year.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "\n",
        "def rename_df_headers(table, dict_headers):\n",
        "    return table.rename(columns=dict_headers)\n",
        "\n",
        "\n",
        "def get_score_sequence_matching(s, c1, category):\n",
        "    if s[c1] is np.nan:\n",
        "        return 0\n",
        "    return difflib.SequenceMatcher(None, s[c1], category).ratio()\n",
        "\n",
        "\n",
        "def get_empty_attribute_to_remove(table):\n",
        "    headers = []\n",
        "    for header in table.columns:\n",
        "        if table[header].isna().sum() * 100 / len(table) > 50:\n",
        "            headers.append(header)\n",
        "    return headers\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1: Exploration-Description\n",
        "## Pr\u00e9senter une description de chacun des attributs des 3 tables, avec des graphiques pour la visualisation des \n",
        "statistiques descriptives au besoin.\n",
        "\n",
        "### Table journal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: visualisations\n",
        "# fr\u00e9quence des valeurs"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(journal.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\"\"\"\n",
        "issn: identifiant du journal\n",
        "Les valeurs de cet attribut semblent suivre un format particulier tel que: 4 digits - 4 digits\n",
        "\n",
        "journal_name: nom textuel du journal\n",
        "Les valeurs sont textuelles, ne suivant pas de valeurs cat\u00e9gorielles particuli\u00e8re \u00e0 priori.\n",
        "\n",
        "pub_name: nom de l'\u00e9diteur du journal\n",
        "Les valeurs sont textuelles, ne suivant pas de valeurs cat\u00e9gorielles particuli\u00e8re \u00e0 priori.\n",
        "\n",
        "is_hybrid: indique si le journal est hybride, ce qui signifie que c'est une revue sur abonnement dont certains articles\n",
        "sont en libre acc\u00e8s, comme l'indique le site http://flourishoa.org/about.\n",
        "\n",
        "category: renseigne sur la/les cat\u00e9gorie(s) des sujets abord\u00e9s par le journal \n",
        "Les valeurs sont textuelles et sont cat\u00e9gorielles. Chaque objet peut poss\u00e9der des valeurs multiples pour cet attribut. La s\u00e9paration entre les diff\u00e9rentes valeurs semblent\n",
        "\u00eatre inconsistante.\n",
        "\n",
        "url: indique l'adresse url du site du journal\n",
        "\n",
        "\n",
        "Les attributs journal_name, pub_name et category \u00e9tant des donn\u00e9es textuelles tr\u00e8s inconsistantes, je d\u00e9cide avant tout\n",
        "traitement et \u00e9tude suppl\u00e9mentaire de transformer les valeurs en minuscule pour limiter au maximum l'inconsistence.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "lowercase_columns(journal, ['journal_name', 'pub_name', 'category'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f\"Valeurs uniques des attributs de journal:\\n\"\n",
        "      f\"{get_uniqueness_attributes(journal)}\")\n",
        "print(f\"Ratio de valeurs vides pour les attributs de journal:\\n\"\n",
        "      f\"{get_ratio_missing_values(journal)}\")\n",
        "\n",
        "print(f\"Valeurs possibles pour l'attribut is_hybrid de journal:\\n\"\n",
        "      f\"{get_unique_values_of_attribute(journal, 'is_hybrid')}\")\n",
        "print(f\"Valeurs possibles pour l'attribut category de journal:\\n\"\n",
        "      f\"{get_unique_values_of_attribute(journal, 'category')}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les attributs category et url pr\u00e9sentent un nombre cons\u00e9quent de valeurs manquantes.\n",
        "L'attribut issn pr\u00e9sente des valeurs uniques pour chacun de ses objets.\n",
        "\n",
        "On remarque qu'il existe uniquement deux valeurs pour l'attribut is_hybrid (soit 1 soit 0). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f\"Valeurs possibles de pub_name quand is_hybrid vaut 1:\\n\"\n",
        "      f\"{journal[journal['is_hybrid'] == 1]['pub_name'].unique()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "plot_categories_frequency(journal, 'pub_name')\n",
        "plot_categories_frequency(journal, 'category')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(price.head())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "price: information du prix d'une publication pour le journal associ\u00e9 \u00e0 une date pr\u00e9cise, en dollar US\n",
        "Si celui-ci est \u00e0 0, on peut coompendre que celui-ci est gratuit\n",
        "\n",
        "date_stamp: horodatage de l'information de prix d'une publication, en format ann\u00e9es-mois-jour\n",
        "\n",
        "journal_id: identifiant du journal\n",
        "Les valeurs semblent suivre consistantement un format du type: 4 digits - 4 digits\n",
        "\n",
        "influence_id: identifiant de l'influence\n",
        "Les valeurs suivent un format 4 digits.\n",
        "\n",
        "url: indique l'adresse url du site de l'auteur\n",
        "\n",
        "license: indique le type de license utilis\u00e9 par le journal pour les diff\u00e9rents articles utilis\u00e9s.\n",
        "\n",
        "\n",
        "On convertit l'attribut date_stamp en type date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "price['date_stamp'] = pd.to_datetime(price['date_stamp'], errors='coerce', format='%Y-%m-%d')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f\"Valeurs uniques des attributs de price:\\n\"\n",
        "      f\"{get_uniqueness_attributes(price)}\")\n",
        "print(f\"Ratio de valeurs vides pour les attributs de price:\\n\"\n",
        "      f\"{get_ratio_missing_values(price)}\")\n",
        "\n",
        "print(f\"Exemples de valeurs possibles pour l'attribut influence_id de price:\\n\"\n",
        "      f\"{get_unique_values_of_attribute(price, 'influence_id')[1:8]}\")\n",
        "print(f\"Valeurs possibles pour l'attribut license de price:\\n\"\n",
        "      f\"{get_unique_values_of_attribute(price, 'license')}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les attributs influence_id, url et license pr\u00e9sentent une majorit\u00e9 de valeurs manquantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "mean_price_per_year = get_mean_price_per_year()\n",
        "\n",
        "plt.bar(range(len(mean_price_per_year)), mean_price_per_year.values())\n",
        "plt.xticks(range(len(mean_price_per_year)), mean_price_per_year.keys())\n",
        "plt.title(\"Moyenne par ann\u00e9e des prix des publications\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table influence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(influence.head())\n",
        "\n",
        "# TODO: proj_ai moyenne"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "journal_name: nom textuel du journal\n",
        "Les valeurs sont textuelles, ne suivant pas de valeurs cat\u00e9gorielles particuli\u00e8re \u00e0 priori.\n",
        "\n",
        "issn: identifiant du journal\n",
        "Les valeurs de cet attribut semblent suivre un format particulier tel que: 4 digits - 4 digits\n",
        "\n",
        "citation_count_sum: indique le nombre de citations du jounal\n",
        "\n",
        "paper_count_sum: indique le nombre de citations des articles du jounal\n",
        "\n",
        "avg_cites_per_paper: indique la moyenne des citations par papier qui sont contenus du journal\n",
        "\n",
        "proj_ai: information sur le score d'influence des articles du journal\n",
        "\n",
        "proj_ai_year: sp\u00e9cification de l'ann\u00e9e o\u00f9 l'information sur le score d'influence des articles du journal a \u00e9t\u00e9 \u00e9tablie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f\"Valeurs uniques des attributs de influence:\\n\"\n",
        "      f\"{get_uniqueness_attributes(influence)}\")\n",
        "print(f\"Ratio de valeurs vides pour les attributs de influence:\\n\"\n",
        "      f\"{get_ratio_missing_values(influence)}\")\n",
        "\n",
        "print(f\"Valeurs possibles pour l'attribut proj_ai_year de influence:\\n\"\n",
        "      f\"{get_unique_values_of_attribute(influence, 'proj_ai_year')}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'attribut proj_ai_year ne pr\u00e9sentant qu'une seule valeur nous indique que les valeurs de l'attribut proj_ai ont toutes\n",
        "\u00e9t\u00e9 \u00e9tablies \u00e0 la m\u00eame p\u00e9riode. \n",
        "\"\"\"\n",
        "\n",
        "# TODO: proj_ai sum in function of journal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2: Pr\u00e9traitement-Repr\u00e9sentation\n",
        "## A. Effectuer un pr\u00e9traitement des donn\u00e9es pour supprimer les duplications et corriger les incoh\u00e9rences s\u2019il y en a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table journal\n",
        "Dans un premier temps, on \u00e9limine les objets pr\u00e9sentant des objets dupliqu\u00e9s sur tous les attributs.\n",
        "On se base sur l'attribut issn qui devrait \u00eatre unique pour chaque objet de la table, on v\u00e9rifie son unicit\u00e9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "nb = len(journal)\n",
        "journal = get_df_duplicated_rows_dropped(journal)\n",
        "print(f\"Nombre d'objets dupliqu\u00e9s \u00e9limin\u00e9s dans journal: {nb - len(journal)}\")\n",
        "\n",
        "check = np.logical_not(journal['issn'].duplicated().any())\n",
        "print(f\"Unicit\u00e9 de l'attribut issn dans la table journal: {check}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table price\n",
        "Etant donn\u00e9 que les index \u00e9taient fournis dans le fichier original et qu'on les utilise afin d'indexer nos objets, \n",
        "on v\u00e9rifie qu'il n'existe pas de duplicata.\n",
        "\n",
        "Ensuite, on \u00e9limine les objets pr\u00e9sentant des objets dupliqu\u00e9s sur tous les attributs.\n",
        "Dans un second temps, dans la table price, les objets se doivent d'\u00eatre uniques selon deux attributs, date_stamp et\n",
        "journal_id. S'ils ne le sont pas, alors ceux sont des objets dupliqu\u00e9s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "check = np.logical_not(price.index.duplicated().any())\n",
        "print(f\"Unicit\u00e9 des indexes de la table price: {check}\")\n",
        "\n",
        "nb = len(price)\n",
        "price = get_df_duplicated_rows_dropped(price)\n",
        "print(f\"Nombre d'objets dupliqu\u00e9s \u00e9limin\u00e9s dans price: {nb - len(price)}\")\n",
        "\n",
        "duplicated_rows = price[price[['date_stamp', 'journal_id']].duplicated(keep=False)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il existe des duplicata ambigus que l'on d\u00e9cide de traiter un \u00e0 un.\n",
        "\n",
        "Premier cas: un des objets pr\u00e9sente un prix nul, on d\u00e9cide de choisir de l'\u00e9liminer au profit de l'autre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(duplicated_rows.iloc[0].fillna(0) == duplicated_rows.iloc[1].fillna(0))\n",
        "price = price.drop(duplicated_rows.index.values[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deuxi\u00e8me cas: leur valeur du prix est diff\u00e9rente d'un l\u00e9ger \u00e9cart, on d\u00e9cide de garder la deuxi\u00e8me de mani\u00e8re\n",
        "arbitraire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(duplicated_rows.iloc[40].fillna(0) == duplicated_rows.iloc[41].fillna(0))\n",
        "price = price.drop(duplicated_rows.index.values[40])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Troisi\u00e8me cas: seule la valeur de license est diff\u00e9rente, on d\u00e9cide de garder la premi\u00e8re de mani\u00e8re arbitraire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for i in range(3, 39, 2):\n",
        "    price = price.drop(duplicated_rows.index.values[i])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table influence\n",
        "Etant donn\u00e9 que les index \u00e9taient fournis dans le fichier original et qu'on les utilise afin d'indexer nos objets, \n",
        "on v\u00e9rifie qu'il n'existe pas de duplicata.\n",
        "\n",
        "On se base sur l'attribut issn qui devrait \u00eatre unique pour chaque objet de la table, on v\u00e9rifie son unicit\u00e9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "nb = len(influence)\n",
        "influence = get_df_duplicated_rows_dropped(influence)\n",
        "print(f\"Nombre d'objets dupliqu\u00e9s \u00e9limin\u00e9s dans influence: {nb - len(influence)}\")\n",
        "\n",
        "check = np.logical_not(influence['issn'].duplicated().any())\n",
        "print(f\"Unicit\u00e9 de l'attribut issn dans la table influence: {check}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merge\n",
        "Afin de simplifier les op\u00e9rations, on g\u00e9n\u00e8re une seule table reprenant les informations des trois tables.\n",
        "On v\u00e9rifie d'abord si les identifiants communs aux diff\u00e9rentes tables sont pr\u00e9sentes dans les tables \u00e0 merger.\n",
        "En premier, on v\u00e9rifie si les valeurs de l'attribut issn de influence sont existantes dans l'attribut du m\u00eame nom \n",
        "dans journal.\n",
        "De m\u00eame, on v\u00e9rifie les valeurs de l'attribut journal_id de price sont existantes dans l'attribut issn dans journal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "check = influence['issn'].isin(journal['issn']).any()\n",
        "print(f\"Pas de valeur d'issn manquante dans journal par rapport \u00e0 influence : {check}\")\n",
        "\n",
        "check = price['journal_id'].isin(journal['issn']).any()\n",
        "print(f\"Pas de valeur d'issn manquante dans journal par rapport \u00e0 price : {check}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On applique maintenant le merge des trois tables en deux \u00e9tapes. D'abord, on merge influence dans journal, puis price\n",
        "est ensuite merg\u00e9 dans le r\u00e9sultat du premier merge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "price = rename_df_headers(price, {\"journal_id\": \"issn\", \"url\": \"url_autor\"})\n",
        "journal = rename_df_headers(journal, {\"url\": \"url_journal\"})\n",
        "\n",
        "temp = pd.merge(journal, influence, on='issn', how='outer')\n",
        "check = len(temp[temp['journal_name_x'] != temp['journal_name_y']])\n",
        "print(f\"Nombre d'aberrances entre les valeurs journal_name des tables journal et influence: {check}\")\n",
        "temp = temp.drop(columns=['journal_name_y'])\n",
        "temp = rename_df_headers(temp, {\"journal_name_x\": \"journal_name\"})\n",
        "\n",
        "print(f\"Valeurs uniques des attributs de temp:\\n\"\n",
        "      f\"{get_uniqueness_attributes(temp)}\")\n",
        "\n",
        "data = pd.merge(temp, price, on=['issn'], how='outer')\n",
        "data = get_df_duplicated_rows_dropped(data)\n",
        "\n",
        "print(f\"Valeurs uniques des attributs de data:\\n\"\n",
        "      f\"{get_uniqueness_attributes(data)}\")\n",
        "print(f\"Ratio de valeurs vides pour les attributs de data:\\n\"\n",
        "      f\"{get_ratio_missing_values(data)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On s'assure bien que les valeurs de l'attribut issn de la nouvelle date (data) sont uniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Y-a-t-il une corr\u00e9lation entre les cat\u00e9gories de journaux (attribut category) et les co\u00fbts de publication \n",
        "(attribut price)? Justifier la r\u00e9ponse.\n",
        "\n",
        "Afin de d\u00e9terminer s'il existe une corr\u00e9lation entre les cat\u00e9gories et l'attribut prix, on s'int\u00e9resse \u00e0 chaque \n",
        "cat\u00e9gorie une \u00e0 une. \n",
        "Etant donn\u00e9 que chaque objet peut avoir plusieurs valeurs de cat\u00e9gories, on d\u00e9cide de s\u00e9parer les cat\u00e9gories selon les \n",
        "diff\u00e9rents s\u00e9parateurs observ\u00e9s (|, and, .). On les convertit ensuite en one hot.\n",
        "On calcule ensuite la corr\u00e9lation cat\u00e9gorie par cat\u00e9gorie avec l'attribut prix. Pour cela, on ne consid\u00e8re que les \n",
        "objets pr\u00e9sentant la cat\u00e9gorie test\u00e9e et les valeurs de prix associ\u00e9es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_labelled_data = data[data['category'].notna()]\n",
        "cat_data_to_predict = data[data['category'].isna()]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_labelled_data['category'] = cat_labelled_data['category'].str.replace(r'[\\.\\|&] | [\\.\\|&] | and ', '.', regex=True)\n",
        "category_dummies = cat_labelled_data['category'].str.get_dummies(sep='.')\n",
        "category_dummies_prefix = category_dummies.add_prefix('category_')\n",
        "print(f'Nombre de cat\u00e9gories apr\u00e8s s\u00e9paration: {category_dummies.shape[1]}')\n",
        "# %%\n",
        "\n",
        "cat_labelled_data = pd.concat([cat_labelled_data, category_dummies_prefix], axis=1) \\\n",
        "    # .drop(columns=['category'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "categories_correlation = {}\n",
        "\n",
        "for header in category_dummies_prefix.columns:\n",
        "    corr = cat_labelled_data[header].corr(cat_labelled_data['price'])\n",
        "    if abs(corr) > 0.1:\n",
        "        categories_correlation[header] = corr\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(categories_correlation.keys(), categories_correlation.values())\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.title(f'Cat\u00e9gories pr\u00e9sentant des corr\u00e9lations fortes avec\\nl\\'attribut prix et leurs valeurs')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque que certaines cat\u00e9gories pr\u00e9sentent effectement une l\u00e9g\u00e8re corr\u00e9lation avec l'attribut prix.\n",
        "(Les cat\u00e9gories pr\u00e9sentant une corr\u00e9lation inf\u00e9rieures \u00e0 0.1 ne sont pas incluses dans le graphe)\n",
        "Les cat\u00e9gories pr\u00e9sentant la plus forte corr\u00e9lation sont 'cell biology' et 'molecular'.\n",
        "Cependant, cette corr\u00e9lation remarqu\u00e9e est tr\u00e8s faible et peut \u00eatre n\u00e9gligeable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Construire un mod\u00e8le pour pr\u00e9dire les valeurs de cat\u00e9gorie de journaux manquantes de la fa\u00e7on la plus pr\u00e9cise\n",
        "possible (cela inclut la s\u00e9lection d\u2019attributs informatifs, le choix et le param\u00e9trage d\u2019un mod\u00e8le de classification,\n",
        "le calcul du score du mod\u00e8le, l\u2019application du mod\u00e8le pour pr\u00e9dire les cat\u00e9gories manquantes). Justifier les choix\n",
        "effectu\u00e9s.\n",
        "TODO: remove attributes price and citations from 2.C and ajust justification\n",
        "Dans le but de pr\u00e9dire les cat\u00e9gories de journaux, on doit s'int\u00e9resser \u00e0 plusieurs attributs qui pourraient nous\n",
        "aider. Le nom du journal pourrait inclure certains mots-cl\u00e9s qui pourraient s'apparenter aux cat\u00e9gories du journal.\n",
        "Le nom de l'\u00e9diteur pourrait \u00e9galement apporter de l'information sur les cat\u00e9gories.\n",
        "Etant donn\u00e9 qu'on a pu trouver certaines corr\u00e9lations entre l'attribut prix et les cat\u00e9gories, on prend \u00e9galement en\n",
        "compte ce param\u00e8tre.\n",
        "Les informations de citation du journal pourraient \u00e9galement se r\u00e9v\u00e9ler porteuses d'informations, ainsi que l'influence\n",
        "des articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_labelled_data = cat_labelled_data[cat_labelled_data['price'].notna()]\n",
        "headers = ['citation_count_sum', 'paper_count_sum', 'avg_cites_per_paper', 'proj_ai', 'price']\n",
        "cat_labelled_data = cat_labelled_data.dropna(axis=0, subset=headers)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for header in tqdm(category_dummies.columns):\n",
        "    cat_labelled_data['jn_' + header] = cat_labelled_data.apply(partial(get_score_sequence_matching, c1='journal_name',\n",
        "                                                                        category=header), axis=1)\n",
        "    cat_labelled_data['pn_' + header] = cat_labelled_data.apply(partial(get_score_sequence_matching, c1='pub_name',\n",
        "                                                                        category=header), axis=1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_labelled_data = cat_labelled_data.drop(columns=['category'])\n",
        "print(f'size cat_labelled_data before splitting: {cat_labelled_data.shape[0]}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "jn_sm_headers = cat_labelled_data.filter(like='jn_').columns.to_list()\n",
        "pn_sm_headers = cat_labelled_data.filter(like='pn_').columns.to_list()\n",
        "# TODO: add date_stamp to attributes_of_interest\n",
        "attributes_of_interest = ['citation_count_sum', 'paper_count_sum', 'avg_cites_per_paper', 'proj_ai', 'price',\n",
        "                          # 'date_stamp',\n",
        "                          ]\n",
        "attributes_of_interest.extend(jn_sm_headers)\n",
        "attributes_of_interest.extend(pn_sm_headers)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrainement\n",
        "On applique des mod\u00e8les de classification ayant la capacit\u00e9 de pouvoir pr\u00e9duire des labels multiples.\n",
        "Pour cela, on utilise la m\u00e9thode MultiOutputClassifier de sklearn afin qui consiste \u00e0 adapter un classificateur par\n",
        "cible.\n",
        "A partir de l\u00e0, on a pu essayer plusieurs types de classification, les deux meilleurs se sont r\u00e9v\u00e9l\u00e9s \u00eatre les random\n",
        "forest et les K plus proches voisins.\n",
        "\n",
        "Le code suivant sert \u00e0 faire une recherche d'hyperparam\u00e8tres (succinte) sur un classification random forest.\n",
        "Pour le confort du temps de compilation, je n'ai pas int\u00e9gr\u00e9 au rendu le classification K plus proche voisin, cependant\n",
        "le r\u00e9sultat du meilleur mod\u00e8le trouv\u00e9 est d\u00e9crit ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "clfs = {'RandomForestClassifier': RandomForestClassifier()}\n",
        "\n",
        "best_model = {'name': '', 'score': 0, 'model': None}\n",
        "for name, clf in clfs.items():\n",
        "    # for i in range(15, 17): # TODO\n",
        "    for i in range(13, 14):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(cat_labelled_data[attributes_of_interest],\n",
        "                                                            cat_labelled_data[category_dummies_prefix.columns],\n",
        "                                                            test_size=0.33, random_state=42)\n",
        "        X_train = X_train[attributes_of_interest]\n",
        "        X_test = X_test[attributes_of_interest]\n",
        "        y_train = y_train.to_numpy()\n",
        "\n",
        "        clf.set_params(max_depth=i)\n",
        "\n",
        "        print(f'Mod\u00e8le {name} {i}')\n",
        "        print('-- Entrainement')\n",
        "        classifier = MultiOutputClassifier(clf, n_jobs=-1)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        train_score = classifier.score(X_train, y_train)\n",
        "        print(f'Score d\\'entra\u00eenement: {train_score}')\n",
        "\n",
        "        print('-- Test')\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        test_score = classifier.score(X_test, y_test)\n",
        "        print(f'Score de test: {test_score}')\n",
        "\n",
        "        if test_score > best_model.get('score'):\n",
        "            best_model['name'], best_model['score'], best_model['model'] = name + ' ' + str(i), test_score, classifier\n",
        "\n",
        "print(f\"Le mod\u00e8le pr\u00e9sentant le meilleur score est {best_model.get('name')} avec {best_model.get('score')}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "R\u00e9sultats des diff\u00e9rents essais:\n",
        "\n",
        "Mod\u00e8le RandomForestClassifier 12\n",
        "-- Entrainement\n",
        "Score d'entra\u00eenement: 0.9916259595254711\n",
        "-- Test\n",
        "Score de test: 0.7312588401697313\n",
        "\n",
        "Mod\u00e8le RandomForestClassifier 13\n",
        "-- Entrainement\n",
        "Score d'entra\u00eenement: 0.994417306350314\n",
        "-- Test\n",
        "Score de test: 0.7369165487977369"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On conclut ainsi que le classificateur random forest ayant une profondeur maximale de 13 pr\u00e9sente des r\u00e9sultats se\n",
        "trouve \u00eatre le plus performant.\n",
        "Aussi, on se trouve en pr\u00e9sence de r\u00e9sultats tr\u00e8s performants, \u00e9tant donn\u00e9 qu'on dispose de 88 labels \u00e0 pr\u00e9dire qui \n",
        "sont les cat\u00e9gories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pr\u00e9dictions\n",
        "On effectue maintenant les pr\u00e9dictions sur les objets pr\u00e9sentant les cat\u00e9gories manquantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_data_to_predict = cat_data_to_predict.dropna(axis=0, subset=headers)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for header in tqdm(category_dummies.columns):\n",
        "    cat_data_to_predict['jn_' + header] = cat_data_to_predict.apply(\n",
        "        partial(get_score_sequence_matching, c1='journal_name',\n",
        "                category=header), axis=1)\n",
        "    cat_data_to_predict['pn_' + header] = cat_data_to_predict.apply(partial(get_score_sequence_matching, c1='pub_name',\n",
        "                                                                            category=header), axis=1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "clf = best_model.get('model')\n",
        "predictions = pd.DataFrame(clf.predict(cat_data_to_predict[attributes_of_interest]))\n",
        "predictions.columns = category_dummies_prefix.columns"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "count_categories = {}\n",
        "for header in predictions.columns:\n",
        "    nb = predictions[header].sum()\n",
        "    if nb >= 1:\n",
        "        count_categories[header] = nb"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(count_categories.keys(), count_categories.values())\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.title(f'Somme des cat\u00e9gories pr\u00e9dites par le mod\u00e8le')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# add categories predictions to their objects in table cat_data_to_predict\n",
        "predictions = predictions.set_index(cat_data_to_predict.index.copy())\n",
        "for header in predictions.columns:\n",
        "    cat_data_to_predict[header] = predictions[header]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 3: R\u00e9gression-clustering\n",
        "## A. Supprimer tous les attributs ayant plus de 50% de donn\u00e9es manquantes.\n",
        "\n",
        "On \u00e9tablit les attributs \u00e0 \u00e9liminer selon leur taux de valeurs manquantes avec nos 3 tables originales (journal, price\n",
        "et influence).\n",
        "On utilise nos donn\u00e9es d\u00e9j\u00e0 travaill\u00e9es qui sont dans les tables cat_labelled_data et cat_data_to_predict car celles-ci\n",
        "pr\u00e9sentent toutes les donn\u00e9es dont nous avons besoin. Comme on a des bons r\u00e9sultats de pr\u00e9dictions, on peut se \n",
        "permettre de les utiliser pour la suite du travail. C'est donc sur ces donn\u00e9es que nous allons \u00e9limin\u00e9s ces attributs.\n",
        "On \u00e9tablit alors une nouvelle table (data) avec toutes ces donn\u00e9es et les attributs pr\u00e9sentant trop de valeurs \n",
        "manquantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f'Attributs \u00e0 \u00e9liminer de la table journal: {get_empty_attribute_to_remove(journal)}')\n",
        "print(f'Attributs \u00e0 \u00e9liminer de la table price: {get_empty_attribute_to_remove(price)}')\n",
        "print(f'Attributs \u00e0 \u00e9liminer de la table influence: {get_empty_attribute_to_remove(influence)}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "cat_data_to_predict = cat_data_to_predict.drop(columns='category')\n",
        "data = cat_labelled_data.append(cat_data_to_predict, sort=False)\n",
        "data = data.drop(columns=['url_journal', 'influence_id', 'url_autor', 'license'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Construire un mod\u00e8le pour pr\u00e9dire le co\u00fbt actuel de publication (attribut \u00abprice\u00bb) \u00e0 partir des autres attributs \n",
        "(cela inclut la s\u00e9lection d\u2019attributs informatifs, le choix et le param\u00e9trage d\u2019un mod\u00e8le de r\u00e9gression, le calcul du \n",
        "score du mod\u00e8le, l\u2019application du mod\u00e8le pour pr\u00e9dire les co\u00fbts).Justifier les choix effectu\u00e9s.\n",
        "Lister les 10 revues qui s\u2019\u00e9cartent le plus (en + ou -) de la valeur pr\u00e9dite.\n",
        " \n",
        " \n",
        "Tout d'abord, nous d\u00e9cidons de ne pas utiliser les attributs que l'on a \u00e9limin\u00e9 dans la question 3.A pr\u00e9senter un nombre cons\u00e9quent de valeurs \n",
        "manquantes, que nous avons trouv\u00e9s dans la question 3.A\n",
        " \n",
        "L'attribut date_stamp dans price \u00e9tablissant la date \u00e0 laquelle le prix a \u00e9t\u00e9 mesur\u00e9 semble int\u00e9ressant.\n",
        "\n",
        "On a vu question 2B que l'attribut prix n'est pas fortement corr\u00e9l\u00e9s aux cat\u00e9gories d'un journal, cependant il existait\n",
        "certaines cat\u00e9gories pr\u00e9sentant une certaine corr\u00e9lation non n\u00e9gligeable. On peut reprendre alors les cat\u00e9gories que \n",
        "l'on a transform\u00e9s en one hot \u00e0 la question 2.C. Aussi, \u00e9tant donn\u00e9 les bons r\u00e9sultats du mod\u00e8le de classification des\n",
        "cat\u00e9gories trouv\u00e9 pr\u00e9c\u00e9demment, on envisage d'utiliser \u00e9galement ces objets. \n",
        "L'information d'un journal sur son hybridicit\u00e9 pourrait \u00e9galement faire varier son prix \n",
        "\n",
        "Les attributs de la table influence semblent \u00eatre pertinentes pour la pr\u00e9diction du prix, en effet, tous les \n",
        "informations sur le nombre de citations pourraient se r\u00e9v\u00e9ler int\u00e9ressantes quant \u00e0 la pr\u00e9diction du prix d'un \n",
        "journal. Aussi, l'attribut informant sur le score d'influence du journal pourrait se r\u00e9v\u00e9ler int\u00e9ressant. \n",
        "Etant donn\u00e9 que l'attribut proj_ai_year pr\u00e9sente toujours la m\u00eame valeur, il n'est pas vraiment pertinent de le \n",
        "conserver. \n",
        "\n",
        "### Construction et estimation des performances du mod\u00e8le"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: add date stamp\n",
        "attributes_of_interest = [\n",
        "    #     'date_stamp',\n",
        "    'citation_count_sum', 'paper_count_sum', 'avg_cites_per_paper', 'proj_ai',\n",
        "    'is_hybrid', 'price']\n",
        "attributes_of_interest.extend(category_dummies_prefix.columns)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "price_labelled_data = data[attributes_of_interest]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(price_labelled_data.drop(columns='price'),\n",
        "                                                    price_labelled_data['price'],\n",
        "                                                    test_size=0.33, random_state=42)\n",
        "\n",
        "regr = RandomForestRegressor(max_depth=22, n_estimators=300, n_jobs=-1)\n",
        "print('-- Entrainement')\n",
        "regr.fit(X_train, y_train)\n",
        "train_score = regr.score(X_train, y_train)\n",
        "print(f'Score d\\'entra\u00eenement: {train_score}')\n",
        "\n",
        "print('-- Test')\n",
        "test_predictions = regr.predict(X_test)\n",
        "test_score = regr.score(X_test, y_test)\n",
        "print(f'Score de test: {test_score}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Application du mod\u00e8le"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "predictions = pd.DataFrame(regr.predict(price_labelled_data.drop(columns='price')))\n",
        "predictions = predictions.set_index(data.index.copy())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "difference_pred_real = dict()\n",
        "for index, p in predictions.iterrows():\n",
        "    difference_pred_real[data['journal_name'][index]] = abs(p[0] - data['price'][index])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(f'Journaux dont leur pr\u00e9diction du prix s\\'\u00e9loigne le plus de la r\u00e9alit\u00e9:\\n')\n",
        "worst_predictions = np.array(heapq.nlargest(10, difference_pred_real, key=difference_pred_real.get))\n",
        "worst_predictions_values = []\n",
        "for p in worst_predictions:\n",
        "    worst_predictions_values.append(difference_pred_real.get(p))\n",
        "    print(f'{p} : {difference_pred_real.get(p)}')\n",
        "\n",
        "worst_predictions = np.vstack([worst_predictions, worst_predictions_values])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Construire un mod\u00e8le pour grouper les revues selon le co\u00fbt actuel de publication (attribut \"price\") et le score\n",
        "d'influence (attribut \"proj_ai\") (cela inclut la d\u00e9termination du nombre de clusters, le choix et le param\u00e9trage d'un\n",
        "mod\u00e8le de clustering, l'application du mod\u00e8le pour trouver les clusters). Justifier les choix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "attributes_of_interest = ['price', 'proj_ai']\n",
        "data_for_clustering = data[attributes_of_interest]\n",
        "\n",
        "plt.figure()\n",
        "y_pred = KMeans(n_clusters=3, random_state=42).fit_predict(data_for_clustering)\n",
        "plt.scatter(data_for_clustering['price'], data_for_clustering['proj_ai'], c=y_pred)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "estim = OPTICS(min_samples=20)\n",
        "\n",
        "print(f'Entrainement et pr\u00e9diction')\n",
        "y_pred = estim.fit_predict(data_for_clustering)\n",
        "plt.scatter(data_for_clustering['price'], data_for_clustering['proj_ai'], c=y_pred)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}